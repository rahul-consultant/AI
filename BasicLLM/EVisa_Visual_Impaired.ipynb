{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1dfe8f-da0f-4347-a7c2-e8101509f796",
   "metadata": {},
   "source": [
    "## This is automated visa application processing system using AI.\n",
    "\n",
    "##### This project will use multi AI model which will assist visually impaired person in filling Visa. Capture his image and then transform into EVisa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "401befbe-82d0-4f3b-82ab-c1f8eb6051c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SpeechRecognition\n",
      "  Using cached speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.12/site-packages (from SpeechRecognition) (4.13.2)\n",
      "Using cached speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
      "Installing collected packages: SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.14.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from transformers import pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55c8ccb6-d413-4855-beda-c5ef0bf318da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "MODEL = \"gpt-4o-mini\"\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a5e905-fdf7-45cc-a8b0-1b83ccd472c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "def talker(message):\n",
    "    response = openai.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"onyx\",    # Also, try replacing onyx with alloy\n",
    "      input=message\n",
    "    )\n",
    "    \n",
    "    audio_stream = BytesIO(response.content)\n",
    "    audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "    play(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405e7a42-3a3a-4f6d-8ba4-3ada95e66483",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an E Visa application called Eazee. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3b00fe-139b-476a-a1cc-fbae8118d1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from fpdf import FPDF\n",
    "import cv2\n",
    "import gradio as gr\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import time\n",
    "\n",
    "# --- Initialization ---\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "def talker(message):\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",\n",
    "        input=message\n",
    "    )\n",
    "    audio_stream = BytesIO(response.content)\n",
    "    audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "    play(audio)\n",
    "\n",
    "def auto_capture_photo(img_path=\"visa_photo.jpg\"):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Could not open camera\")\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise Exception(\"Failed to capture image\")\n",
    "    cv2.imwrite(img_path, frame)\n",
    "    # Encode image as base64\n",
    "    with open(img_path, \"rb\") as f:\n",
    "        photo_b64 = base64.b64encode(f.read()).decode()\n",
    "    return img_path, photo_b64\n",
    "\n",
    "def create_visa_application_pdf(firstname, lastname, address, gender, img_path):\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=14)\n",
    "    pdf.cell(200, 10, \"E-Visa Application Form\", ln=True, align=\"C\")\n",
    "    pdf.ln(10)\n",
    "    pdf.cell(50, 10, f\"First Name: {firstname}\", ln=True)\n",
    "    pdf.cell(50, 10, f\"Last Name: {lastname}\", ln=True)\n",
    "    pdf.cell(50, 10, f\"Gender: {gender}\", ln=True)\n",
    "    pdf.multi_cell(0, 10, f\"Address: {address}\", align=\"L\")\n",
    "    pdf.ln(10)\n",
    "    pdf.cell(50, 10, \"Applicant Photo:\", ln=True)\n",
    "    if os.path.exists(img_path):\n",
    "        pdf.image(img_path, x=pdf.get_x(), y=pdf.get_y(), w=40)\n",
    "    pdf_path = \"visa_application.pdf\"\n",
    "    pdf.output(pdf_path)\n",
    "    return pdf_path\n",
    "\n",
    "Visa_function = {\n",
    "    \"name\": \"create_visa_application_pdf\",\n",
    "    \"description\": (\n",
    "        \"Generate a visa application PDF. \"\n",
    "        \"Call this whenever you need to create a visa application, for example when a customer asks 'Can you help with my visa application?'\"\n",
    "    ),\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"firstname\": {\"type\": \"string\", \"description\": \"The applicant's first name\"},\n",
    "            \"lastname\": {\"type\": \"string\", \"description\": \"The applicant's last name\"},\n",
    "            \"address\": {\"type\": \"string\", \"description\": \"The applicant's address\"},\n",
    "            \"gender\": {\"type\": \"string\", \"description\": \"The applicant's gender\"},\n",
    "            \"photo\": {\"type\": \"string\", \"description\": \"Base64-encoded image of the applicant's photo\"}\n",
    "        },\n",
    "        \"required\": [\"firstname\", \"lastname\", \"address\", \"gender\", \"photo\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "tools = [{\"type\": \"function\", \"function\": Visa_function}]\n",
    "\n",
    "def handle_tool_call(tool_call):\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    firstname = args.get(\"firstname\")\n",
    "    lastname = args.get(\"lastname\")\n",
    "    address = args.get(\"address\")\n",
    "    gender = args.get(\"gender\")\n",
    "    photo_b64 = args.get(\"photo\")\n",
    "    img_path = \"visa_photo.jpg\"\n",
    "    with open(img_path, \"wb\") as f:\n",
    "        f.write(base64.b64decode(photo_b64))\n",
    "    pdf_path = create_visa_application_pdf(firstname, lastname, address, gender, img_path)\n",
    "    return pdf_path\n",
    "\n",
    "def chat_with_openai(audio_input, history, photo_b64):\n",
    "    user_text = \"\"\n",
    "    if audio_input is not None:\n",
    "        import speech_recognition as sr\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.AudioFile(audio_input) as source:\n",
    "            audio = recognizer.record(source)\n",
    "        try:\n",
    "            user_text = recognizer.recognize_google(audio)\n",
    "            print(user_text)\n",
    "        except Exception:\n",
    "            user_text = \"\"\n",
    "    if not user_text:\n",
    "        user_text = \" \"  # Avoid empty input\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant for E-Visa applications. Collect all required info and call the function when ready.\"}]\n",
    "    messages += history\n",
    "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    reply = response.choices[0].message\n",
    "\n",
    "    # Speak the assistant's reply\n",
    "    if reply.content:\n",
    "        talker(reply.content)\n",
    "\n",
    "    # If OpenAI wants to call the function\n",
    "    if reply.tool_calls:\n",
    "        tool_call = reply.tool_calls[0]\n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "        if not args.get(\"photo\") and not photo_b64:\n",
    "            talker(\"I will now take your photo. Please look at the camera.\")\n",
    "            img_path, photo_b64 = auto_capture_photo()\n",
    "            args[\"photo\"] = photo_b64\n",
    "            pdf_path = handle_tool_call(tool_call)\n",
    "            talker(\"Your visa application is ready. You can download it now.\")\n",
    "            return messages, photo_b64, pdf_path, \"Your visa application is ready. Download below.\", gr.update(value=None)\n",
    "        elif not args.get(\"photo\"):\n",
    "            args[\"photo\"] = photo_b64\n",
    "        pdf_path = handle_tool_call(tool_call)\n",
    "        talker(\"Your visa application is ready. You can download it now.\")\n",
    "        return messages, photo_b64, pdf_path, \"Your visa application is ready. Download below.\", gr.update(value=None)\n",
    "    else:\n",
    "        return messages, photo_b64, None, reply.content, gr.update(value=None)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Eazee: Voice-Driven E-Visa Assistant for the Visually Impaired\")\n",
    "    history = gr.State([])\n",
    "    photo_b64 = gr.State(None)\n",
    "    audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Speak to the assistant\", autoplay=True, streaming=True)\n",
    "    output_text = gr.Textbox(label=\"Assistant\", interactive=False)\n",
    "    pdf_file = gr.File(label=\"Download Visa Application PDF\")\n",
    "\n",
    "    def auto_submit(audio, history, photo_b64):\n",
    "        # Wait a moment to simulate \"pause\" after recording\n",
    "        time.sleep(1.5)\n",
    "        return chat_with_openai(audio, history, photo_b64)\n",
    "\n",
    "    audio_input.change(\n",
    "        auto_submit,\n",
    "        inputs=[audio_input, history, photo_b64],\n",
    "        outputs=[history, photo_b64, pdf_file, output_text, audio_input]\n",
    "    )\n",
    "\n",
    "    # Optionally, add a manual button for fallback\n",
    "    gr.Markdown(\"If you need to repeat, just speak again.\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7b158e0-2e40-4750-a7ef-60b44575c02e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from fpdf import FPDF\n",
    "import cv2\n",
    "import gradio as gr\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from transformers import pipeline\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(filename='debug.log', level=logging.DEBUG, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# --- Initialization ---\n",
    "try:\n",
    "    load_dotenv(override=True)\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    openai = OpenAI(api_key=openai_api_key)\n",
    "    MODEL = \"gpt-4o-mini\"\n",
    "    logging.info(\"Initialization successful.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Initialization failed: {e}\")\n",
    "\n",
    "\n",
    "def transcribeMain(audio):\n",
    "    if audio is None:\n",
    "        return \"Error: No audio input received.\"\n",
    "    \n",
    "    try:\n",
    "        # Load the audio file\n",
    "        with sr.AudioFile(audio) as source:\n",
    "            recognizer.adjust_for_ambient_noise(source)  # Reduce background noise\n",
    "            audio_data = recognizer.record(source)  # Capture the entire audio file\n",
    "        \n",
    "        # Perform speech recognition\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        logging.info(text)   # Using Google Speech Recognition\n",
    "        return text\n",
    "    \n",
    "    except sr.UnknownValueError:\n",
    "        return \"Error: Could not understand the audio.\"\n",
    "    except sr.RequestError:\n",
    "        return \"Error: Could not request results from the speech recognition service.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing audio: {str(e)}\"\n",
    "\n",
    "\n",
    "# Load the speech-to-text model\n",
    "asr_pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "# Define the function to process audio input\n",
    "def transcribe(audio):\n",
    "    if audio is None:\n",
    "        return \"Error: No audio input received.\"\n",
    "    \n",
    "    try:\n",
    "        # Load and process the audio file\n",
    "        audio_data, samplerate = sf.read(audio)\n",
    "        \n",
    "        if not isinstance(audio_data, np.ndarray):\n",
    "            return \"Error: Invalid audio format.\"\n",
    "        \n",
    "        return asr_pipeline(audio)[\"text\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing audio: {str(e)}\"\n",
    "    \n",
    "def talker(message):\n",
    "    try:\n",
    "        response = openai.audio.speech.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=\"onyx\",\n",
    "            input=message\n",
    "        )\n",
    "        audio_stream = BytesIO(response.content)\n",
    "        audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "        play(audio)\n",
    "        logging.info(\"Spoke message successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Talker error: {e}\")\n",
    "\n",
    "def auto_capture_photo(img_path=\"visa_photo.jpg\"):\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            raise Exception(\"Could not open camera\")\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        if not ret:\n",
    "            raise Exception(\"Failed to capture image\")\n",
    "        cv2.imwrite(img_path, frame)\n",
    "        with open(img_path, \"rb\") as f:\n",
    "            photo_b64 = base64.b64encode(f.read()).decode()\n",
    "        logging.info(\"Photo captured and encoded.\")\n",
    "        return img_path, photo_b64\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Photo capture error: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_visa_application_pdf(firstname, lastname, address, gender, img_path):\n",
    "    try:\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", size=14)\n",
    "        pdf.cell(200, 10, \"E-Visa Application Form\", ln=True, align=\"C\")\n",
    "        pdf.ln(10)\n",
    "        pdf.cell(50, 10, f\"First Name: {firstname}\", ln=True)\n",
    "        pdf.cell(50, 10, f\"Last Name: {lastname}\", ln=True)\n",
    "        pdf.cell(50, 10, f\"Gender: {gender}\", ln=True)\n",
    "        pdf.multi_cell(0, 10, f\"Address: {address}\", align=\"L\")\n",
    "        pdf.ln(10)\n",
    "        pdf.cell(50, 10, \"Applicant Photo:\", ln=True)\n",
    "        if os.path.exists(img_path):\n",
    "            pdf.image(img_path, x=pdf.get_x(), y=pdf.get_y(), w=40)\n",
    "        pdf_path = \"visa_application.pdf\"\n",
    "        pdf.output(pdf_path)\n",
    "        logging.info(\"PDF created successfully.\")\n",
    "        return pdf_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"PDF creation error: {e}\")\n",
    "        raise\n",
    "\n",
    "Visa_function = {\n",
    "    \"name\": \"create_visa_application_pdf\",\n",
    "    \"description\": (\n",
    "        \"Generate a visa application PDF. \"\n",
    "        \"Call this whenever you need to create a visa application, for example when a customer asks 'Can you help with my visa application?'\"\n",
    "    ),\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"firstname\": {\"type\": \"string\", \"description\": \"The applicant's first name\"},\n",
    "            \"lastname\": {\"type\": \"string\", \"description\": \"The applicant's last name\"},\n",
    "            \"address\": {\"type\": \"string\", \"description\": \"The applicant's address\"},\n",
    "            \"gender\": {\"type\": \"string\", \"description\": \"The applicant's gender\"},\n",
    "            \"photo\": {\"type\": \"string\", \"description\": \"Base64-encoded image of the applicant's photo\"}\n",
    "        },\n",
    "        \"required\": [\"firstname\", \"lastname\", \"address\", \"gender\", \"photo\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "tools = [{\"type\": \"function\", \"function\": Visa_function}]\n",
    "\n",
    "def handle_tool_call(tool_call):\n",
    "    try:\n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "        firstname = args.get(\"firstname\")\n",
    "        lastname = args.get(\"lastname\")\n",
    "        address = args.get(\"address\")\n",
    "        gender = args.get(\"gender\")\n",
    "        photo_b64 = args.get(\"photo\")\n",
    "        img_path = \"visa_photo.jpg\"\n",
    "        with open(img_path, \"wb\") as f:\n",
    "            f.write(base64.b64decode(photo_b64))\n",
    "        pdf_path = create_visa_application_pdf(firstname, lastname, address, gender, img_path)\n",
    "        logging.info(\"Tool call handled successfully.\")\n",
    "        return pdf_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Tool call error: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- Main Chat Logic ---\n",
    "def chat_with_openai(audio_input, history, photo_b64, collected):\n",
    "    try:\n",
    "        audio_text = transcribeMain(audio_input)\n",
    "        user_text = audio_text\n",
    "        logging.info(\"Phase 1\")\n",
    "        logging.info(user_text)\n",
    "            \n",
    "        if not collected:\n",
    "            collected = {\"firstname\": None, \"lastname\": None, \"address\": None, \"gender\": None}\n",
    "\n",
    "      \n",
    "\n",
    "        text = user_text.lower()\n",
    "        if not collected[\"firstname\"] and \"first name\" in text:\n",
    "            collected[\"firstname\"] = user_text.split(\"first name\")[-1].strip(\":,. \")\n",
    "            print(\"Rahul Search Here : \"+collected[\"firstname\"])\n",
    "            logging.info(\"Phase 4\")\n",
    "            logging.info(\"Rahul Search Here : \"+collected[\"firstname\"])\n",
    "        elif not collected[\"lastname\"] and \"last name\" in text:\n",
    "            collected[\"lastname\"] = user_text.split(\"last name\")[-1].strip(\":,. \")\n",
    "            print(\"Rahul Search Here : \"+collected[\"lastname\"])\n",
    "            logging.info(\"Rahul Search Here : \"+collected[\"lastname\"])\n",
    "        elif not collected[\"address\"] and \"address\" in text:\n",
    "            collected[\"address\"] = user_text.split(\"address\")[-1].strip(\":,. \")\n",
    "            print(\"Rahul Search Here : \"+collected[\"address\"])\n",
    "            logging.info(\"Rahul Search Here : \"+collected[\"address\"])\n",
    "        elif not collected[\"gender\"] and \"gender\" in text:\n",
    "            collected[\"gender\"] = user_text.split(\"gender\")[-1].strip(\":,. \")\n",
    "            print(\"Rahul Search Here : \"+collected[\"gender\"])\n",
    "            logging.info(\"Rahul Search Here : \"+collected[\"gender\"])\n",
    "            logging.debug(f\"Gender extracted: {collected['gender']}\")\n",
    "        elif history:\n",
    "            last_assistant = history[-1][1].lower() if isinstance(history[-1], (list, tuple)) else \"\"\n",
    "            if \"first name\" in last_assistant and not collected[\"firstname\"]:\n",
    "                collected[\"firstname\"] = user_text.strip()\n",
    "                print(\"Rahul Search Here : \"+collected[\"firstname\"])\n",
    "                logging.info(\"Rahul Search Here : \"+collected[\"firstname\"])\n",
    "                logging.info(\"Phase 5\")\n",
    "            elif \"last name\" in last_assistant and not collected[\"lastname\"]:\n",
    "                collected[\"lastname\"] = user_text.strip()\n",
    "                print(\"Rahul Search Here : \"+collected[\"lastname\"])\n",
    "                logging.info(\"Rahul Search Here : \"+collected[\"lastname\"])\n",
    "            elif \"address\" in last_assistant and not collected[\"address\"]:\n",
    "                collected[\"address\"] = user_text.strip()\n",
    "                print(\"Rahul Search Here : \"+collected[\"address\"])\n",
    "                logging.info(\"Rahul Search Here : \"+collected[\"address\"])\n",
    "            elif \"gender\" in last_assistant and not collected[\"gender\"]:\n",
    "                collected[\"gender\"] = user_text.strip()\n",
    "                print(\"Rahul Search Here : \"+collected[\"gender\"])\n",
    "                logging.info(\"Rahul Search Here : \"+collected[\"gender\"])\n",
    "                logging.debug(f\"Gender extracted: {collected['gender']}\")\n",
    "                logging.debug(f\"Gender extracted (history): {collected['gender']}\")\n",
    "                \n",
    "\n",
    "        \n",
    "        if all(collected.values()):\n",
    "            talker(\"Thank you. I will now take your photo. Please look at the camera.\")\n",
    "            print(\"Rahul Photo section Here : \")\n",
    "            logging.info(\"Rahul Photo section Here\")\n",
    "            logging.info(\"Phase 6\")\n",
    "            img_path, photo_b64 = auto_capture_photo()\n",
    "            pdf_path = create_visa_application_pdf(\n",
    "                collected[\"firstname\"], collected[\"lastname\"], collected[\"address\"], collected[\"gender\"], img_path\n",
    "            )\n",
    "            talker(\"Your visa application is ready. You can download it now.\")\n",
    "            logging.info(\"All fields collected and PDF generated.\")\n",
    "            return [], photo_b64, pdf_path, \"Your visa application is ready. Download below.\", collected\n",
    "\n",
    "        missing = [k for k, v in collected.items() if not v]\n",
    "        if not history:\n",
    "            greeting = \"Welcome to Eazee! I will help you fill out your visa application. What is your first name?\"\n",
    "            talker(greeting)\n",
    "            logging.info(\"Started new conversation.\")\n",
    "            return [[user_text, greeting]], photo_b64, None, greeting, collected\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are a helpful assistant for E-Visa applications. \"\n",
    "            \"Collect only first name, last name, address, and gender. \"\n",
    "            \"Do not ask for or mention photo. \"\n",
    "            \"Ask for only one missing field at a time.\"\n",
    "        )\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        for h in history:\n",
    "            if isinstance(h, (list, tuple)) and len(h) == 2:\n",
    "                messages.append({\"role\": \"user\", \"content\": h[0]})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": h[1]})\n",
    "                logging.info(\"Phase 7\")\n",
    "        messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "        response = openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        reply = response.choices[0].message.content\n",
    "        talker(reply)\n",
    "        history.append([user_text, reply])\n",
    "        logging.info(\"Assistant replied.\")\n",
    "        return history, photo_b64, None, reply, collected\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Chat error: {e}\")\n",
    "        return history, photo_b64, None, f\"Error: {e}\", collected\n",
    "\n",
    "# --- Gradio UI ---\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Eazee: Voice-Driven E-Visa Assistant for the Visually Impaired\")\n",
    "    history = gr.State([])\n",
    "    photo_b64 = gr.State(None)\n",
    "    collected = gr.State({})\n",
    "    audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Speak to the assistant\")\n",
    "    \n",
    "    output_text = gr.Textbox(label=\"Assistant\", interactive=False)\n",
    "    pdf_file = gr.File(label=\"Download Visa Application PDF\")\n",
    "    submit = gr.Button(\"Submit/Continue\")\n",
    "    submit.click(\n",
    "        chat_with_openai,\n",
    "        inputs=[audio_input, history, photo_b64, collected],\n",
    "        outputs=[history, photo_b64, pdf_file, output_text, collected]\n",
    "    )\n",
    "\n",
    "demo.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3b92821-a651-498c-a2a9-d8462bb06fd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the speech-to-text model\n",
    "asr_pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "# Define the function to process audio input\n",
    "def transcribe(audio):\n",
    "    if audio is None:\n",
    "        return \"Error: No audio input received.\"\n",
    "    \n",
    "    try:\n",
    "        # Load and process the audio file\n",
    "        audio_data, samplerate = sf.read(audio)\n",
    "        \n",
    "        if not isinstance(audio_data, np.ndarray):\n",
    "            return \"Error: Invalid audio format.\"\n",
    "        \n",
    "        return asr_pipeline(audio)[\"text\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing audio: {str(e)}\"\n",
    "\n",
    "# Create the Gradio interface with both microphone and file upload options\n",
    "audio_input = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\", label=\"Speak or upload an audio file\")\n",
    "interface = gr.Interface(fn=transcribe, inputs=audio_input, outputs=\"text\")\n",
    "\n",
    "# Launch the app\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10ccfe85-acdd-4789-883f-20af08a5dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize the recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to transcribe audio\n",
    "def transcribe(audio):\n",
    "    if audio is None:\n",
    "        return \"Error: No audio input received.\"\n",
    "    \n",
    "    try:\n",
    "        # Load the audio file\n",
    "        with sr.AudioFile(audio) as source:\n",
    "            recognizer.adjust_for_ambient_noise(source)  # Reduce background noise\n",
    "            audio_data = recognizer.record(source)  # Capture the entire audio file\n",
    "        \n",
    "        # Perform speech recognition\n",
    "        text = recognizer.recognize_google(audio_data)  # Using Google Speech Recognition\n",
    "        return text\n",
    "    \n",
    "    except sr.UnknownValueError:\n",
    "        return \"Error: Could not understand the audio.\"\n",
    "    except sr.RequestError:\n",
    "        return \"Error: Could not request results from the speech recognition service.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing audio: {str(e)}\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "audio_input = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\", label=\"Speak or upload an audio file\")\n",
    "interface = gr.Interface(fn=transcribe, inputs=audio_input, outputs=\"text\")\n",
    "\n",
    "# Launch the app\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6e407-e6a2-4b1f-a49c-723f4c1cecb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
